{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1b9131",
   "metadata": {},
   "source": [
    "# 1. PDF Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from bs4 import BeautifulSoup\n",
    "import re, os, json\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"Page\\s+\\d+\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "pdf_path = \"../data/AI Training Document.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "for doc in documents:\n",
    "    doc.page_content = clean_text(doc.page_content)\n",
    "\n",
    "print(f\"Loaded {len(documents)} cleaned documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04583dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "os.makedirs(\"../chunks\", exist_ok=True)\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "with open(\"../chunks/chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunk_texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(chunks)} chunks.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
