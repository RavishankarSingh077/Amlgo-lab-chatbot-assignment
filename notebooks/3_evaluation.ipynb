{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb55e51c",
   "metadata": {},
   "source": [
    "# 3. Evaluation of RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f18801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"../vectordb\", embedding_function=None)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "llm = OllamaLLM(model=\"phi\")\n",
    "prompt_template = \"\"\"\n",
    "You are an intelligent assistant. Use only the context below to answer the user's question.\n",
    "If you do not find the answer in the context, respond with \"I don't know.\"\n",
    "\n",
    "Context:{context}\n",
    "Question:{question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "qa_chain = load_qa_with_sources_chain(llm=llm, chain_type=\"stuff\", prompt=prompt, document_variable_name=\"context\")\n",
    "\n",
    "query = \"What is the purpose of the training mentioned in the document?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "result = qa_chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "print(\"Answer:\", result[\"output_text\"])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
